{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e79e557-c5c7-4f25-8259-2f68e77e4c95",
   "metadata": {},
   "source": [
    "# YOLOR\n",
    "\n",
    "To set up the notebook below, run the following cells. Do not run the get_coco.sh script unless you want to train your own YOLOR model on the COCO dataset.\n",
    "You can incur a charge for going over the data storage limit, so be aware of that if you plan to download the dataset. An alternative dataset one could use is the BCCD dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d46260-ce7f-415f-958b-1d15bfb77655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install gdown to interact with google drive through the terminal. Use it to download the yolor_csp_x.pt pretrained model weights for inference\n",
    "!pip install gdown\n",
    "!gdown https://drive.google.com/uc?id=1NbMG3ivuBQ4S8kEhFJ0FIqOQXevGje_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ff033d-03d7-4441-9cfc-5873a72ad36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to create the coco directory, and download the full coco dataset. \n",
    "# Be aware that this dataset is very large, and training will take a very long time. This is not recommended for anyone running a notebook on a free gpu,\n",
    "# as you will not be able to use coco in this short time period to make a model.\n",
    "\n",
    "# !mkdir coco\n",
    "# !cd coco\n",
    "# !bash ../scripts/get_coco.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef67111-2d23-41e6-ab00-92d3f5be3985",
   "metadata": {},
   "source": [
    "# Inference/Object detection with the pretrained YOLOR CSP X model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ca3650-29ae-442d-b4de-f4881ff4375a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#install youtube dl to get sample videos from youtube, and opencv-python-headless to run detection script on videos in Gradient.\n",
    "!pip install youtube_dl\n",
    "!pip install opencv-python-headless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b9bc42-1b35-49ba-ba3a-b058f6e54b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the inputvid.mp4 video\n",
    "\n",
    "import youtube_dl\n",
    "\n",
    "url = 'https://www.youtube.com/watch?v=b8QZJ5ZodTs'\n",
    "    \n",
    "url_list = [url]\n",
    "youtube_dl.YoutubeDL({'outtmpl': 'inference/images/inputvid.mp4', 'format_id': 'worstvideo/worst', 'format': '160', 'vcodec': 'utf-8'}).download(url_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "619baa34-7987-423c-8227-0784dcad9af3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-28T00:47:24.425000Z",
     "iopub.status.busy": "2022-04-28T00:47:24.424368Z",
     "iopub.status.idle": "2022-04-28T00:47:46.336762Z",
     "shell.execute_reply": "2022-04-28T00:47:46.336405Z",
     "shell.execute_reply.started": "2022-04-28T00:47:24.424977Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(agnostic_nms=False, augment=False, cfg='cfg/yolor_csp_x.cfg', classes=None, conf_thres=0.25, device='0', img_size=1280, iou_thres=0.5, names='data/coco.names', output='inference/output', save_txt=False, source='inference/images/inputvid.mp4', update=False, view_img=False, weights=['yolor_csp_x.pt'])\n",
      "/opt/conda/lib/python3.8/site-packages/torch/functional.py:568: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  /opt/pytorch/pytorch/aten/src/ATen/native/TensorShape.cpp:2166.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "video 1/1 (1/414) /notebooks/inference/images/inputvid.mp4: 768x1280 15 persons, 1 backpacks, Done. (0.073s)\n",
      "video 1/1 (2/414) /notebooks/inference/images/inputvid.mp4: 768x1280 15 persons, 1 handbags, Done. (0.022s)\n",
      "video 1/1 (3/414) /notebooks/inference/images/inputvid.mp4: 768x1280 12 persons, 1 backpacks, 1 handbags, Done. (0.021s)\n",
      "video 1/1 (4/414) /notebooks/inference/images/inputvid.mp4: 768x1280 13 persons, 1 backpacks, 1 handbags, Done. (0.021s)\n",
      "video 1/1 (5/414) /notebooks/inference/images/inputvid.mp4: 768x1280 14 persons, 1 backpacks, 1 handbags, Done. (0.021s)\n",
      "video 1/1 (6/414) /notebooks/inference/images/inputvid.mp4: 768x1280 14 persons, 1 handbags, Done. (0.021s)\n",
      "video 1/1 (7/414) /notebooks/inference/images/inputvid.mp4: 768x1280 14 persons, 1 handbags, Done. (0.021s)\n",
      "video 1/1 (8/414) /notebooks/inference/images/inputvid.mp4: 768x1280 12 persons, 1 handbags, Done. (0.022s)\n",
      "video 1/1 (9/414) /notebooks/inference/images/inputvid.mp4: 768x1280 11 persons, 1 handbags, Done. (0.022s)\n",
      "video 1/1 (10/414) /notebooks/inference/images/inputvid.mp4: 768x1280 10 persons, 1 handbags, Done. (0.021s)\n",
      "video 1/1 (11/414) /notebooks/inference/images/inputvid.mp4: 768x1280 8 persons, 1 handbags, Done. (0.021s)\n",
      "video 1/1 (12/414) /notebooks/inference/images/inputvid.mp4: 768x1280 8 persons, 1 handbags, Done. (0.021s)\n",
      "video 1/1 (13/414) /notebooks/inference/images/inputvid.mp4: 768x1280 9 persons, 1 handbags, Done. (0.022s)\n",
      "video 1/1 (14/414) /notebooks/inference/images/inputvid.mp4: 768x1280 8 persons, 1 handbags, 1 ovens, Done. (0.021s)\n",
      "video 1/1 (15/414) /notebooks/inference/images/inputvid.mp4: 768x1280 7 persons, Done. (0.022s)\n",
      "video 1/1 (16/414) /notebooks/inference/images/inputvid.mp4: 768x1280 7 persons, 1 handbags, Done. (0.021s)\n",
      "video 1/1 (17/414) /notebooks/inference/images/inputvid.mp4: 768x1280 10 persons, 1 handbags, Done. (0.021s)\n",
      "video 1/1 (18/414) /notebooks/inference/images/inputvid.mp4: 768x1280 8 persons, 1 handbags, Done. (0.021s)\n",
      "video 1/1 (19/414) /notebooks/inference/images/inputvid.mp4: 768x1280 7 persons, Done. (0.021s)\n",
      "video 1/1 (20/414) /notebooks/inference/images/inputvid.mp4: 768x1280 5 persons, 1 handbags, Done. (0.021s)\n",
      "video 1/1 (21/414) /notebooks/inference/images/inputvid.mp4: 768x1280 5 persons, 1 handbags, Done. (0.021s)\n",
      "video 1/1 (22/414) /notebooks/inference/images/inputvid.mp4: 768x1280 6 persons, Done. (0.021s)\n",
      "video 1/1 (23/414) /notebooks/inference/images/inputvid.mp4: 768x1280 5 persons, Done. (0.021s)\n",
      "video 1/1 (24/414) /notebooks/inference/images/inputvid.mp4: 768x1280 8 persons, Done. (0.021s)\n",
      "video 1/1 (25/414) /notebooks/inference/images/inputvid.mp4: 768x1280 5 persons, Done. (0.021s)\n",
      "video 1/1 (26/414) /notebooks/inference/images/inputvid.mp4: 768x1280 5 persons, Done. (0.021s)\n",
      "video 1/1 (27/414) /notebooks/inference/images/inputvid.mp4: 768x1280 6 persons, Done. (0.022s)\n",
      "video 1/1 (28/414) /notebooks/inference/images/inputvid.mp4: 768x1280 7 persons, Done. (0.021s)\n",
      "video 1/1 (29/414) /notebooks/inference/images/inputvid.mp4: 768x1280 6 persons, Done. (0.021s)\n",
      "video 1/1 (30/414) /notebooks/inference/images/inputvid.mp4: 768x1280 7 persons, Done. (0.028s)\n",
      "video 1/1 (31/414) /notebooks/inference/images/inputvid.mp4: 768x1280 7 persons, 1 traffic lights, Done. (0.021s)\n",
      "video 1/1 (32/414) /notebooks/inference/images/inputvid.mp4: 768x1280 9 persons, 1 handbags, Done. (0.021s)\n",
      "video 1/1 (33/414) /notebooks/inference/images/inputvid.mp4: 768x1280 6 persons, Done. (0.021s)\n",
      "video 1/1 (34/414) /notebooks/inference/images/inputvid.mp4: 768x1280 5 persons, Done. (0.021s)\n",
      "video 1/1 (35/414) /notebooks/inference/images/inputvid.mp4: 768x1280 4 persons, Done. (0.021s)\n",
      "video 1/1 (36/414) /notebooks/inference/images/inputvid.mp4: 768x1280 4 persons, Done. (0.021s)\n",
      "video 1/1 (37/414) /notebooks/inference/images/inputvid.mp4: 768x1280 2 persons, Done. (0.021s)\n",
      "video 1/1 (38/414) /notebooks/inference/images/inputvid.mp4: 768x1280 3 persons, 1 toilets, Done. (0.021s)\n",
      "video 1/1 (39/414) /notebooks/inference/images/inputvid.mp4: 768x1280 3 persons, Done. (0.021s)\n",
      "video 1/1 (40/414) /notebooks/inference/images/inputvid.mp4: 768x1280 2 persons, Done. (0.021s)\n",
      "video 1/1 (41/414) /notebooks/inference/images/inputvid.mp4: 768x1280 2 persons, Done. (0.021s)\n",
      "video 1/1 (42/414) /notebooks/inference/images/inputvid.mp4: 768x1280 2 persons, 1 refrigerators, Done. (0.021s)\n",
      "video 1/1 (43/414) /notebooks/inference/images/inputvid.mp4: 768x1280 3 persons, 1 refrigerators, Done. (0.021s)\n",
      "video 1/1 (44/414) /notebooks/inference/images/inputvid.mp4: 768x1280 3 persons, Done. (0.021s)\n",
      "video 1/1 (45/414) /notebooks/inference/images/inputvid.mp4: 768x1280 3 persons, Done. (0.021s)\n",
      "video 1/1 (46/414) /notebooks/inference/images/inputvid.mp4: 768x1280 1 persons, Done. (0.021s)\n",
      "video 1/1 (47/414) /notebooks/inference/images/inputvid.mp4: 768x1280 2 persons, 1 refrigerators, Done. (0.021s)\n",
      "video 1/1 (48/414) /notebooks/inference/images/inputvid.mp4: 768x1280 1 persons, Done. (0.021s)\n",
      "video 1/1 (49/414) /notebooks/inference/images/inputvid.mp4: 768x1280 1 persons, Done. (0.021s)\n",
      "video 1/1 (50/414) /notebooks/inference/images/inputvid.mp4: 768x1280 1 persons, Done. (0.021s)\n",
      "video 1/1 (51/414) /notebooks/inference/images/inputvid.mp4: 768x1280 1 persons, Done. (0.021s)\n",
      "video 1/1 (52/414) /notebooks/inference/images/inputvid.mp4: 768x1280 2 persons, Done. (0.022s)\n",
      "video 1/1 (53/414) /notebooks/inference/images/inputvid.mp4: 768x1280 2 persons, Done. (0.021s)\n",
      "video 1/1 (54/414) /notebooks/inference/images/inputvid.mp4: 768x1280 Done. (0.022s)\n",
      "video 1/1 (55/414) /notebooks/inference/images/inputvid.mp4: 768x1280 1 cars, 1 trucks, Done. (0.023s)\n",
      "video 1/1 (56/414) /notebooks/inference/images/inputvid.mp4: 768x1280 2 persons, 1 cars, 1 trucks, Done. (0.022s)\n",
      "video 1/1 (57/414) /notebooks/inference/images/inputvid.mp4: 768x1280 1 suitcases, Done. (0.022s)\n",
      "video 1/1 (58/414) /notebooks/inference/images/inputvid.mp4: 768x1280 1 cars, Done. (0.022s)\n",
      "video 1/1 (59/414) /notebooks/inference/images/inputvid.mp4: 768x1280 Done. (0.021s)\n",
      "video 1/1 (60/414) /notebooks/inference/images/inputvid.mp4: 768x1280 Done. (0.021s)\n",
      "video 1/1 (61/414) /notebooks/inference/images/inputvid.mp4: 768x1280 2 suitcases, Done. (0.022s)\n",
      "video 1/1 (62/414) /notebooks/inference/images/inputvid.mp4: 768x1280 1 traffic lights, 1 suitcases, Done. (0.021s)\n",
      "video 1/1 (63/414) /notebooks/inference/images/inputvid.mp4: 768x1280 2 traffic lights, 1 suitcases, Done. (0.021s)\n",
      "video 1/1 (64/414) /notebooks/inference/images/inputvid.mp4: 768x1280 2 suitcases, Done. (0.021s)\n",
      "video 1/1 (65/414) /notebooks/inference/images/inputvid.mp4: 768x1280 2 persons, Done. (0.021s)\n",
      "video 1/1 (66/414) /notebooks/inference/images/inputvid.mp4: 768x1280 2 persons, Done. (0.021s)\n",
      "video 1/1 (67/414) /notebooks/inference/images/inputvid.mp4: 768x1280 1 persons, 1 traffic lights, Done. (0.021s)\n",
      "video 1/1 (68/414) /notebooks/inference/images/inputvid.mp4: 768x1280 2 persons, 1 cars, Done. (0.021s)\n",
      "video 1/1 (69/414) /notebooks/inference/images/inputvid.mp4: 768x1280 2 persons, 1 cars, Done. (0.021s)\n",
      "video 1/1 (70/414) /notebooks/inference/images/inputvid.mp4: 768x1280 2 persons, 2 cars, Done. (0.021s)\n",
      "video 1/1 (71/414) /notebooks/inference/images/inputvid.mp4: 768x1280 2 persons, Done. (0.022s)\n",
      "video 1/1 (72/414) /notebooks/inference/images/inputvid.mp4: 768x1280 1 persons, Done. (0.021s)\n",
      "video 1/1 (73/414) /notebooks/inference/images/inputvid.mp4: 768x1280 1 traffic lights, Done. (0.021s)\n",
      "video 1/1 (74/414) /notebooks/inference/images/inputvid.mp4: 768x1280 1 persons, Done. (0.021s)\n",
      "video 1/1 (75/414) /notebooks/inference/images/inputvid.mp4: 768x1280 1 persons, Done. (0.022s)\n",
      "video 1/1 (76/414) /notebooks/inference/images/inputvid.mp4: 768x1280 2 persons, Done. (0.021s)\n",
      "video 1/1 (77/414) /notebooks/inference/images/inputvid.mp4: 768x1280 Done. (0.021s)\n",
      "video 1/1 (78/414) /notebooks/inference/images/inputvid.mp4: 768x1280 1 persons, Done. (0.021s)\n",
      "video 1/1 (79/414) /notebooks/inference/images/inputvid.mp4: 768x1280 1 cows, Done. (0.022s)\n",
      "video 1/1 (80/414) /notebooks/inference/images/inputvid.mp4: 768x1280 Done. (0.020s)\n",
      "video 1/1 (81/414) /notebooks/inference/images/inputvid.mp4: 768x1280 Done. (0.021s)\n",
      "video 1/1 (82/414) /notebooks/inference/images/inputvid.mp4: 768x1280 1 persons, Done. (0.021s)\n",
      "video 1/1 (83/414) /notebooks/inference/images/inputvid.mp4: 768x1280 3 persons, 1 suitcases, Done. (0.021s)\n",
      "video 1/1 (84/414) /notebooks/inference/images/inputvid.mp4: 768x1280 2 persons, Done. (0.022s)\n",
      "video 1/1 (85/414) /notebooks/inference/images/inputvid.mp4: 768x1280 2 persons, Done. (0.021s)\n",
      "video 1/1 (86/414) /notebooks/inference/images/inputvid.mp4: 768x1280 2 persons, Done. (0.021s)\n",
      "video 1/1 (87/414) /notebooks/inference/images/inputvid.mp4: 768x1280 2 persons, Done. (0.021s)\n",
      "video 1/1 (88/414) /notebooks/inference/images/inputvid.mp4: 768x1280 2 persons, Done. (0.021s)\n",
      "video 1/1 (89/414) /notebooks/inference/images/inputvid.mp4: 768x1280 3 persons, Done. (0.021s)\n",
      "video 1/1 (90/414) /notebooks/inference/images/inputvid.mp4: 768x1280 3 persons, Done. (0.021s)\n",
      "video 1/1 (91/414) /notebooks/inference/images/inputvid.mp4: 768x1280 3 persons, Done. (0.021s)\n",
      "video 1/1 (92/414) /notebooks/inference/images/inputvid.mp4: 768x1280 4 persons, Done. (0.021s)\n",
      "video 1/1 (93/414) /notebooks/inference/images/inputvid.mp4: 768x1280 5 persons, 1 refrigerators, Done. (0.022s)\n",
      "video 1/1 (94/414) /notebooks/inference/images/inputvid.mp4: 768x1280 3 persons, Done. (0.022s)\n",
      "video 1/1 (95/414) /notebooks/inference/images/inputvid.mp4: 768x1280 3 persons, Done. (0.022s)\n",
      "video 1/1 (96/414) /notebooks/inference/images/inputvid.mp4: 768x1280 3 persons, Done. (0.021s)\n",
      "video 1/1 (97/414) /notebooks/inference/images/inputvid.mp4: 768x1280 2 persons, Done. (0.021s)\n",
      "video 1/1 (98/414) /notebooks/inference/images/inputvid.mp4: 768x1280 4 persons, Done. (0.021s)\n",
      "video 1/1 (99/414) /notebooks/inference/images/inputvid.mp4: 768x1280 3 persons, Done. (0.022s)\n",
      "video 1/1 (100/414) /notebooks/inference/images/inputvid.mp4: 768x1280 3 persons, Done. (0.022s)\n",
      "video 1/1 (101/414) /notebooks/inference/images/inputvid.mp4: 768x1280 3 persons, Done. (0.022s)\n",
      "video 1/1 (102/414) /notebooks/inference/images/inputvid.mp4: 768x1280 3 persons, Done. (0.022s)\n",
      "video 1/1 (103/414) /notebooks/inference/images/inputvid.mp4: 768x1280 3 persons, Done. (0.021s)\n",
      "video 1/1 (104/414) /notebooks/inference/images/inputvid.mp4: 768x1280 4 persons, Done. (0.021s)\n",
      "video 1/1 (105/414) /notebooks/inference/images/inputvid.mp4: 768x1280 4 persons, Done. (0.022s)\n",
      "video 1/1 (106/414) /notebooks/inference/images/inputvid.mp4: 768x1280 3 persons, Done. (0.024s)\n",
      "video 1/1 (107/414) /notebooks/inference/images/inputvid.mp4: 768x1280 5 persons, Done. (0.021s)\n",
      "video 1/1 (108/414) /notebooks/inference/images/inputvid.mp4: 768x1280 3 persons, Done. (0.021s)\n",
      "video 1/1 (109/414) /notebooks/inference/images/inputvid.mp4: 768x1280 5 persons, Done. (0.021s)\n",
      "video 1/1 (110/414) /notebooks/inference/images/inputvid.mp4: 768x1280 3 persons, Done. (0.021s)\n",
      "video 1/1 (111/414) /notebooks/inference/images/inputvid.mp4: 768x1280 5 persons, Done. (0.021s)\n",
      "video 1/1 (112/414) /notebooks/inference/images/inputvid.mp4: 768x1280 4 persons, Done. (0.021s)\n",
      "video 1/1 (113/414) /notebooks/inference/images/inputvid.mp4: 768x1280 5 persons, Done. (0.021s)\n",
      "video 1/1 (114/414) /notebooks/inference/images/inputvid.mp4: 768x1280 5 persons, Done. (0.021s)\n",
      "video 1/1 (115/414) /notebooks/inference/images/inputvid.mp4: 768x1280 3 persons, 1 cars, Done. (0.021s)\n",
      "video 1/1 (116/414) /notebooks/inference/images/inputvid.mp4: 768x1280 2 persons, 1 suitcases, Done. (0.021s)\n",
      "video 1/1 (117/414) /notebooks/inference/images/inputvid.mp4: 768x1280 4 persons, Done. (0.021s)\n",
      "video 1/1 (118/414) /notebooks/inference/images/inputvid.mp4: 768x1280 5 persons, Done. (0.021s)\n",
      "video 1/1 (119/414) /notebooks/inference/images/inputvid.mp4: 768x1280 4 persons, Done. (0.021s)\n",
      "video 1/1 (120/414) /notebooks/inference/images/inputvid.mp4: 768x1280 3 persons, Done. (0.021s)\n",
      "video 1/1 (121/414) /notebooks/inference/images/inputvid.mp4: 768x1280 3 persons, Done. (0.021s)\n",
      "video 1/1 (122/414) /notebooks/inference/images/inputvid.mp4: 768x1280 3 persons, Done. (0.021s)\n",
      "video 1/1 (123/414) /notebooks/inference/images/inputvid.mp4: 768x1280 3 persons, Done. (0.021s)\n",
      "video 1/1 (124/414) /notebooks/inference/images/inputvid.mp4: 768x1280 3 persons, Done. (0.021s)\n",
      "video 1/1 (125/414) /notebooks/inference/images/inputvid.mp4: 768x1280 3 persons, Done. (0.021s)\n",
      "video 1/1 (126/414) /notebooks/inference/images/inputvid.mp4: 768x1280 4 persons, Done. (0.021s)\n",
      "video 1/1 (127/414) /notebooks/inference/images/inputvid.mp4: 768x1280 5 persons, Done. (0.022s)\n",
      "video 1/1 (128/414) /notebooks/inference/images/inputvid.mp4: 768x1280 6 persons, 1 umbrellas, Done. (0.021s)\n",
      "video 1/1 (129/414) /notebooks/inference/images/inputvid.mp4: 768x1280 7 persons, 1 umbrellas, Done. (0.021s)\n",
      "video 1/1 (130/414) /notebooks/inference/images/inputvid.mp4: 768x1280 7 persons, Done. (0.021s)\n",
      "video 1/1 (131/414) /notebooks/inference/images/inputvid.mp4: 768x1280 9 persons, Done. (0.021s)\n",
      "video 1/1 (132/414) /notebooks/inference/images/inputvid.mp4: 768x1280 6 persons, Done. (0.021s)\n",
      "video 1/1 (133/414) /notebooks/inference/images/inputvid.mp4: 768x1280 7 persons, Done. (0.021s)\n",
      "video 1/1 (134/414) /notebooks/inference/images/inputvid.mp4: 768x1280 7 persons, Done. (0.022s)\n",
      "video 1/1 (135/414) /notebooks/inference/images/inputvid.mp4: 768x1280 10 persons, Done. (0.022s)\n",
      "video 1/1 (136/414) /notebooks/inference/images/inputvid.mp4: 768x1280 9 persons, Done. (0.021s)\n",
      "video 1/1 (137/414) /notebooks/inference/images/inputvid.mp4: 768x1280 9 persons, Done. (0.022s)\n",
      "video 1/1 (138/414) /notebooks/inference/images/inputvid.mp4: 768x1280 10 persons, Done. (0.021s)\n",
      "video 1/1 (139/414) /notebooks/inference/images/inputvid.mp4: 768x1280 11 persons, Done. (0.021s)\n",
      "video 1/1 (140/414) /notebooks/inference/images/inputvid.mp4: 768x1280 8 persons, Done. (0.021s)\n",
      "video 1/1 (141/414) /notebooks/inference/images/inputvid.mp4: 768x1280 8 persons, Done. (0.021s)\n",
      "video 1/1 (142/414) /notebooks/inference/images/inputvid.mp4: 768x1280 6 persons, Done. (0.021s)\n",
      "video 1/1 (143/414) /notebooks/inference/images/inputvid.mp4: 768x1280 6 persons, Done. (0.021s)\n",
      "video 1/1 (144/414) /notebooks/inference/images/inputvid.mp4: 768x1280 5 persons, Done. (0.021s)\n",
      "video 1/1 (145/414) /notebooks/inference/images/inputvid.mp4: 768x1280 7 persons, Done. (0.021s)\n",
      "video 1/1 (146/414) /notebooks/inference/images/inputvid.mp4: 768x1280 7 persons, Done. (0.022s)\n",
      "video 1/1 (147/414) /notebooks/inference/images/inputvid.mp4: 768x1280 6 persons, Done. (0.021s)\n",
      "video 1/1 (148/414) /notebooks/inference/images/inputvid.mp4: 768x1280 5 persons, Done. (0.023s)\n",
      "video 1/1 (149/414) /notebooks/inference/images/inputvid.mp4: 768x1280 6 persons, Done. (0.022s)\n",
      "video 1/1 (150/414) /notebooks/inference/images/inputvid.mp4: 768x1280 6 persons, Done. (0.022s)\n",
      "video 1/1 (151/414) /notebooks/inference/images/inputvid.mp4: 768x1280 5 persons, Done. (0.022s)\n",
      "video 1/1 (152/414) /notebooks/inference/images/inputvid.mp4: 768x1280 6 persons, Done. (0.022s)\n",
      "video 1/1 (153/414) /notebooks/inference/images/inputvid.mp4: 768x1280 4 persons, Done. (0.022s)\n",
      "video 1/1 (154/414) /notebooks/inference/images/inputvid.mp4: 768x1280 5 persons, Done. (0.022s)\n",
      "video 1/1 (155/414) /notebooks/inference/images/inputvid.mp4: 768x1280 5 persons, Done. (0.022s)\n",
      "video 1/1 (156/414) /notebooks/inference/images/inputvid.mp4: 768x1280 9 persons, Done. (0.022s)\n",
      "video 1/1 (157/414) /notebooks/inference/images/inputvid.mp4: 768x1280 7 persons, Done. (0.022s)\n",
      "video 1/1 (158/414) /notebooks/inference/images/inputvid.mp4: 768x1280 6 persons, Done. (0.023s)\n",
      "video 1/1 (159/414) /notebooks/inference/images/inputvid.mp4: 768x1280 8 persons, Done. (0.022s)\n",
      "video 1/1 (160/414) /notebooks/inference/images/inputvid.mp4: 768x1280 6 persons, 1 parking meters, 1 suitcases, Done. (0.022s)\n",
      "video 1/1 (161/414) /notebooks/inference/images/inputvid.mp4: 768x1280 13 persons, 1 traffic lights, Done. (0.022s)\n",
      "video 1/1 (162/414) /notebooks/inference/images/inputvid.mp4: 768x1280 13 persons, 1 traffic lights, 1 chairs, Done. (0.022s)\n",
      "video 1/1 (163/414) /notebooks/inference/images/inputvid.mp4: 768x1280 10 persons, 1 traffic lights, 1 chairs, Done. (0.022s)\n",
      "video 1/1 (164/414) /notebooks/inference/images/inputvid.mp4: 768x1280 10 persons, 1 traffic lights, 1 chairs, Done. (0.022s)\n",
      "video 1/1 (165/414) /notebooks/inference/images/inputvid.mp4: 768x1280 10 persons, 1 traffic lights, 1 suitcases, 1 chairs, Done. (0.022s)\n",
      "video 1/1 (166/414) /notebooks/inference/images/inputvid.mp4: 768x1280 10 persons, 1 traffic lights, 1 suitcases, Done. (0.023s)\n",
      "video 1/1 (167/414) /notebooks/inference/images/inputvid.mp4: 768x1280 9 persons, 1 traffic lights, 1 suitcases, Done. (0.022s)\n",
      "video 1/1 (168/414) /notebooks/inference/images/inputvid.mp4: 768x1280 10 persons, 1 traffic lights, 1 handbags, 1 suitcases, Done. (0.022s)\n",
      "video 1/1 (169/414) /notebooks/inference/images/inputvid.mp4: 768x1280 10 persons, 1 traffic lights, Done. (0.022s)\n",
      "video 1/1 (170/414) /notebooks/inference/images/inputvid.mp4: 768x1280 10 persons, 1 traffic lights, Done. (0.022s)\n",
      "video 1/1 (171/414) /notebooks/inference/images/inputvid.mp4: 768x1280 9 persons, 1 traffic lights, Done. (0.022s)\n",
      "video 1/1 (172/414) /notebooks/inference/images/inputvid.mp4: 768x1280 12 persons, 1 traffic lights, Done. (0.023s)\n",
      "video 1/1 (173/414) /notebooks/inference/images/inputvid.mp4: 768x1280 13 persons, 1 traffic lights, Done. (0.022s)\n",
      "video 1/1 (174/414) /notebooks/inference/images/inputvid.mp4: 768x1280 13 persons, 1 traffic lights, Done. (0.022s)\n",
      "video 1/1 (175/414) /notebooks/inference/images/inputvid.mp4: 768x1280 10 persons, 1 traffic lights, Done. (0.022s)\n",
      "video 1/1 (176/414) /notebooks/inference/images/inputvid.mp4: 768x1280 9 persons, 1 traffic lights, Done. (0.022s)\n",
      "video 1/1 (177/414) /notebooks/inference/images/inputvid.mp4: 768x1280 8 persons, 1 traffic lights, 1 handbags, Done. (0.021s)\n",
      "video 1/1 (178/414) /notebooks/inference/images/inputvid.mp4: 768x1280 11 persons, 1 traffic lights, Done. (0.021s)\n",
      "video 1/1 (179/414) /notebooks/inference/images/inputvid.mp4: 768x1280 9 persons, 1 traffic lights, Done. (0.021s)\n",
      "video 1/1 (180/414) /notebooks/inference/images/inputvid.mp4: 768x1280 10 persons, 1 traffic lights, Done. (0.021s)\n",
      "video 1/1 (181/414) /notebooks/inference/images/inputvid.mp4: 768x1280 12 persons, 1 traffic lights, Done. (0.022s)\n",
      "video 1/1 (182/414) /notebooks/inference/images/inputvid.mp4: 768x1280 11 persons, 1 traffic lights, Done. (0.021s)\n",
      "video 1/1 (183/414) /notebooks/inference/images/inputvid.mp4: 768x1280 11 persons, 1 traffic lights, Done. (0.021s)\n",
      "video 1/1 (184/414) /notebooks/inference/images/inputvid.mp4: 768x1280 9 persons, 1 traffic lights, Done. (0.021s)\n",
      "video 1/1 (185/414) /notebooks/inference/images/inputvid.mp4: 768x1280 8 persons, 1 traffic lights, Done. (0.021s)\n",
      "video 1/1 (186/414) /notebooks/inference/images/inputvid.mp4: 768x1280 9 persons, 1 traffic lights, Done. (0.021s)\n",
      "video 1/1 (187/414) /notebooks/inference/images/inputvid.mp4: 768x1280 9 persons, 2 traffic lights, Done. (0.021s)\n",
      "video 1/1 (188/414) /notebooks/inference/images/inputvid.mp4: 768x1280 9 persons, 2 traffic lights, Done. (0.021s)\n",
      "video 1/1 (189/414) /notebooks/inference/images/inputvid.mp4: 768x1280 8 persons, 1 traffic lights, Done. (0.021s)\n",
      "video 1/1 (190/414) /notebooks/inference/images/inputvid.mp4: 768x1280 8 persons, 1 traffic lights, Done. (0.021s)\n",
      "video 1/1 (191/414) /notebooks/inference/images/inputvid.mp4: 768x1280 7 persons, 1 traffic lights, Done. (0.022s)\n",
      "video 1/1 (192/414) /notebooks/inference/images/inputvid.mp4: 768x1280 8 persons, 1 traffic lights, Done. (0.021s)\n",
      "video 1/1 (193/414) /notebooks/inference/images/inputvid.mp4: 768x1280 9 persons, 1 traffic lights, Done. (0.021s)\n",
      "video 1/1 (194/414) /notebooks/inference/images/inputvid.mp4: 768x1280 7 persons, 1 traffic lights, 1 wine glasss, Done. (0.021s)\n",
      "video 1/1 (195/414) /notebooks/inference/images/inputvid.mp4: 768x1280 7 persons, 1 traffic lights, Done. (0.021s)\n",
      "video 1/1 (196/414) /notebooks/inference/images/inputvid.mp4: 768x1280 7 persons, 1 traffic lights, Done. (0.022s)\n",
      "video 1/1 (197/414) /notebooks/inference/images/inputvid.mp4: 768x1280 8 persons, 1 traffic lights, Done. (0.021s)\n",
      "video 1/1 (198/414) /notebooks/inference/images/inputvid.mp4: 768x1280 5 persons, 1 traffic lights, Done. (0.021s)\n",
      "video 1/1 (199/414) /notebooks/inference/images/inputvid.mp4: 768x1280 8 persons, 1 traffic lights, Done. (0.025s)\n",
      "video 1/1 (200/414) /notebooks/inference/images/inputvid.mp4: 768x1280 7 persons, 1 traffic lights, Done. (0.021s)\n",
      "video 1/1 (201/414) /notebooks/inference/images/inputvid.mp4: 768x1280 6 persons, 1 traffic lights, Done. (0.021s)\n",
      "video 1/1 (202/414) /notebooks/inference/images/inputvid.mp4: 768x1280 5 persons, 1 traffic lights, Done. (0.021s)\n",
      "video 1/1 (203/414) /notebooks/inference/images/inputvid.mp4: 768x1280 8 persons, 1 traffic lights, Done. (0.021s)\n",
      "video 1/1 (204/414) /notebooks/inference/images/inputvid.mp4: 768x1280 8 persons, 1 traffic lights, Done. (0.021s)\n",
      "video 1/1 (205/414) /notebooks/inference/images/inputvid.mp4: 768x1280 8 persons, 1 traffic lights, Done. (0.021s)\n",
      "video 1/1 (206/414) /notebooks/inference/images/inputvid.mp4: 768x1280 6 persons, 1 traffic lights, Done. (0.021s)\n",
      "video 1/1 (207/414) /notebooks/inference/images/inputvid.mp4: 768x1280 7 persons, 2 traffic lights, Done. (0.021s)\n",
      "video 1/1 (208/414) /notebooks/inference/images/inputvid.mp4: 768x1280 7 persons, 1 traffic lights, 1 chairs, Done. (0.021s)\n",
      "video 1/1 (209/414) /notebooks/inference/images/inputvid.mp4: 768x1280 7 persons, 1 traffic lights, Done. (0.021s)\n",
      "video 1/1 (210/414) /notebooks/inference/images/inputvid.mp4: 768x1280 8 persons, 1 traffic lights, Done. (0.021s)\n",
      "video 1/1 (211/414) /notebooks/inference/images/inputvid.mp4: 768x1280 5 persons, 2 traffic lights, Done. (0.022s)\n",
      "video 1/1 (212/414) /notebooks/inference/images/inputvid.mp4: 768x1280 9 persons, 1 traffic lights, Done. (0.021s)\n",
      "video 1/1 (213/414) /notebooks/inference/images/inputvid.mp4: 768x1280 8 persons, 1 traffic lights, Done. (0.021s)\n",
      "video 1/1 (214/414) /notebooks/inference/images/inputvid.mp4: 768x1280 9 persons, 1 traffic lights, Done. (0.021s)\n",
      "video 1/1 (215/414) /notebooks/inference/images/inputvid.mp4: 768x1280 9 persons, 1 traffic lights, Done. (0.021s)\n",
      "video 1/1 (216/414) /notebooks/inference/images/inputvid.mp4: 768x1280 7 persons, 1 traffic lights, Done. (0.021s)\n",
      "video 1/1 (217/414) /notebooks/inference/images/inputvid.mp4: 768x1280 9 persons, 1 cars, 1 traffic lights, Done. (0.021s)\n",
      "video 1/1 (218/414) /notebooks/inference/images/inputvid.mp4: 768x1280 10 persons, 1 traffic lights, Done. (0.021s)\n",
      "video 1/1 (219/414) /notebooks/inference/images/inputvid.mp4: 768x1280 6 persons, 1 traffic lights, 1 chairs, Done. (0.021s)\n",
      "video 1/1 (220/414) /notebooks/inference/images/inputvid.mp4: 768x1280 8 persons, 1 traffic lights, Done. (0.022s)\n",
      "video 1/1 (221/414) /notebooks/inference/images/inputvid.mp4: 768x1280 10 persons, 1 traffic lights, Done. (0.021s)\n",
      "video 1/1 (222/414) /notebooks/inference/images/inputvid.mp4: 768x1280 9 persons, 1 traffic lights, Done. (0.021s)\n",
      "video 1/1 (223/414) /notebooks/inference/images/inputvid.mp4: 768x1280 10 persons, 1 traffic lights, Done. (0.021s)\n",
      "video 1/1 (224/414) /notebooks/inference/images/inputvid.mp4: 768x1280 11 persons, 1 traffic lights, Done. (0.021s)\n",
      "video 1/1 (225/414) /notebooks/inference/images/inputvid.mp4: 768x1280 11 persons, 1 traffic lights, Done. (0.021s)\n",
      "video 1/1 (226/414) /notebooks/inference/images/inputvid.mp4: 768x1280 10 persons, 2 traffic lights, 1 cups, Done. (0.021s)\n",
      "video 1/1 (227/414) /notebooks/inference/images/inputvid.mp4: 768x1280 7 persons, 2 traffic lights, 1 cups, Done. (0.021s)\n",
      "video 1/1 (228/414) /notebooks/inference/images/inputvid.mp4: 768x1280 8 persons, Done. (0.021s)\n",
      "video 1/1 (229/414) /notebooks/inference/images/inputvid.mp4: 768x1280 6 persons, Done. (0.021s)\n",
      "video 1/1 (230/414) /notebooks/inference/images/inputvid.mp4: 768x1280 7 persons, 1 handbags, Done. (0.021s)\n",
      "video 1/1 (231/414) /notebooks/inference/images/inputvid.mp4: 768x1280 6 persons, Done. (0.021s)\n",
      "video 1/1 (232/414) /notebooks/inference/images/inputvid.mp4: 768x1280 4 persons, Done. (0.021s)\n",
      "video 1/1 (233/414) /notebooks/inference/images/inputvid.mp4: 768x1280 6 persons, 1 handbags, Done. (0.021s)\n",
      "video 1/1 (234/414) /notebooks/inference/images/inputvid.mp4: 768x1280 8 persons, 1 handbags, Done. (0.021s)\n",
      "video 1/1 (235/414) /notebooks/inference/images/inputvid.mp4: 768x1280 5 persons, Done. (0.021s)\n",
      "video 1/1 (236/414) /notebooks/inference/images/inputvid.mp4: 768x1280 3 persons, Done. (0.021s)\n",
      "video 1/1 (237/414) /notebooks/inference/images/inputvid.mp4: 768x1280 2 persons, 1 cars, Done. (0.021s)\n",
      "video 1/1 (238/414) /notebooks/inference/images/inputvid.mp4: 768x1280 5 persons, Done. (0.021s)\n",
      "video 1/1 (239/414) /notebooks/inference/images/inputvid.mp4: 768x1280 4 persons, Done. (0.022s)\n",
      "video 1/1 (240/414) /notebooks/inference/images/inputvid.mp4: 768x1280 4 persons, Done. (0.021s)\n",
      "video 1/1 (241/414) /notebooks/inference/images/inputvid.mp4: 768x1280 5 persons, Done. (0.021s)\n",
      "video 1/1 (242/414) /notebooks/inference/images/inputvid.mp4: 768x1280 5 persons, Done. (0.021s)\n",
      "video 1/1 (243/414) /notebooks/inference/images/inputvid.mp4: 768x1280 4 persons, 1 tvs, Done. (0.021s)\n",
      "video 1/1 (244/414) /notebooks/inference/images/inputvid.mp4: 768x1280 3 persons, Done. (0.021s)\n",
      "video 1/1 (245/414) /notebooks/inference/images/inputvid.mp4: 768x1280 3 persons, 1 fire hydrants, Done. (0.021s)\n",
      "video 1/1 (246/414) /notebooks/inference/images/inputvid.mp4: 768x1280 2 persons, 1 fire hydrants, Done. (0.021s)\n",
      "video 1/1 (247/414) /notebooks/inference/images/inputvid.mp4: 768x1280 4 persons, 1 bottles, Done. (0.021s)\n",
      "video 1/1 (248/414) /notebooks/inference/images/inputvid.mp4: 768x1280 3 persons, 1 bottles, Done. (0.021s)\n",
      "video 1/1 (249/414) /notebooks/inference/images/inputvid.mp4: 768x1280 3 persons, Done. (0.021s)\n",
      "video 1/1 (250/414) /notebooks/inference/images/inputvid.mp4: 768x1280 4 persons, Done. (0.021s)\n",
      "video 1/1 (251/414) /notebooks/inference/images/inputvid.mp4: 768x1280 6 persons, Done. (0.021s)\n",
      "video 1/1 (252/414) /notebooks/inference/images/inputvid.mp4: 768x1280 4 persons, Done. (0.028s)\n",
      "video 1/1 (253/414) /notebooks/inference/images/inputvid.mp4: 768x1280 4 persons, Done. (0.021s)\n",
      "video 1/1 (254/414) /notebooks/inference/images/inputvid.mp4: 768x1280 4 persons, Done. (0.021s)\n",
      "video 1/1 (255/414) /notebooks/inference/images/inputvid.mp4: 768x1280 4 persons, Done. (0.021s)\n",
      "video 1/1 (256/414) /notebooks/inference/images/inputvid.mp4: 768x1280 6 persons, Done. (0.021s)\n",
      "video 1/1 (257/414) /notebooks/inference/images/inputvid.mp4: 768x1280 6 persons, Done. (0.021s)\n",
      "video 1/1 (258/414) /notebooks/inference/images/inputvid.mp4: 768x1280 5 persons, Done. (0.021s)\n",
      "video 1/1 (259/414) /notebooks/inference/images/inputvid.mp4: 768x1280 7 persons, 1 suitcases, Done. (0.021s)\n",
      "video 1/1 (260/414) /notebooks/inference/images/inputvid.mp4: 768x1280 4 persons, 1 suitcases, Done. (0.021s)\n",
      "video 1/1 (261/414) /notebooks/inference/images/inputvid.mp4: 768x1280 5 persons, 1 suitcases, Done. (0.021s)\n",
      "video 1/1 (262/414) /notebooks/inference/images/inputvid.mp4: 768x1280 4 persons, Done. (0.021s)\n",
      "video 1/1 (263/414) /notebooks/inference/images/inputvid.mp4: 768x1280 4 persons, 3 suitcases, Done. (0.021s)\n",
      "video 1/1 (264/414) /notebooks/inference/images/inputvid.mp4: 768x1280 5 persons, Done. (0.021s)\n",
      "video 1/1 (265/414) /notebooks/inference/images/inputvid.mp4: 768x1280 5 persons, Done. (0.021s)\n",
      "video 1/1 (266/414) /notebooks/inference/images/inputvid.mp4: 768x1280 3 persons, Done. (0.021s)\n",
      "video 1/1 (267/414) /notebooks/inference/images/inputvid.mp4: 768x1280 3 persons, Done. (0.021s)\n",
      "video 1/1 (268/414) /notebooks/inference/images/inputvid.mp4: 768x1280 5 persons, Done. (0.021s)\n",
      "video 1/1 (269/414) /notebooks/inference/images/inputvid.mp4: 768x1280 5 persons, 1 bottles, Done. (0.021s)\n",
      "video 1/1 (270/414) /notebooks/inference/images/inputvid.mp4: 768x1280 5 persons, 1 bottles, Done. (0.021s)\n",
      "video 1/1 (271/414) /notebooks/inference/images/inputvid.mp4: 768x1280 4 persons, Done. (0.021s)\n",
      "video 1/1 (272/414) /notebooks/inference/images/inputvid.mp4: 768x1280 3 persons, Done. (0.022s)\n",
      "video 1/1 (273/414) /notebooks/inference/images/inputvid.mp4: 768x1280 4 persons, Done. (0.021s)\n",
      "video 1/1 (274/414) /notebooks/inference/images/inputvid.mp4: 768x1280 6 persons, Done. (0.021s)\n",
      "video 1/1 (275/414) /notebooks/inference/images/inputvid.mp4: 768x1280 4 persons, Done. (0.021s)\n",
      "video 1/1 (276/414) /notebooks/inference/images/inputvid.mp4: 768x1280 4 persons, Done. (0.021s)\n",
      "video 1/1 (277/414) /notebooks/inference/images/inputvid.mp4: 768x1280 4 persons, Done. (0.021s)\n",
      "video 1/1 (278/414) /notebooks/inference/images/inputvid.mp4: 768x1280 3 persons, Done. (0.021s)\n",
      "video 1/1 (279/414) /notebooks/inference/images/inputvid.mp4: 768x1280 2 persons, Done. (0.021s)\n",
      "video 1/1 (280/414) /notebooks/inference/images/inputvid.mp4: 768x1280 2 persons, Done. (0.021s)\n",
      "video 1/1 (281/414) /notebooks/inference/images/inputvid.mp4: 768x1280 3 persons, Done. (0.021s)\n",
      "video 1/1 (282/414) /notebooks/inference/images/inputvid.mp4: 768x1280 3 persons, 1 cars, Done. (0.021s)\n",
      "video 1/1 (283/414) /notebooks/inference/images/inputvid.mp4: 768x1280 2 persons, 1 horses, Done. (0.021s)\n",
      "video 1/1 (284/414) /notebooks/inference/images/inputvid.mp4: 768x1280 2 persons, Done. (0.021s)\n",
      "video 1/1 (285/414) /notebooks/inference/images/inputvid.mp4: 768x1280 2 persons, Done. (0.021s)\n",
      "video 1/1 (286/414) /notebooks/inference/images/inputvid.mp4: 768x1280 3 persons, Done. (0.021s)\n",
      "video 1/1 (287/414) /notebooks/inference/images/inputvid.mp4: 768x1280 3 persons, Done. (0.021s)\n",
      "video 1/1 (288/414) /notebooks/inference/images/inputvid.mp4: 768x1280 2 persons, Done. (0.024s)\n",
      "video 1/1 (289/414) /notebooks/inference/images/inputvid.mp4: 768x1280 5 persons, Done. (0.022s)\n",
      "video 1/1 (290/414) /notebooks/inference/images/inputvid.mp4: 768x1280 4 persons, Done. (0.021s)\n",
      "video 1/1 (291/414) /notebooks/inference/images/inputvid.mp4: 768x1280 4 persons, Done. (0.021s)\n",
      "video 1/1 (292/414) /notebooks/inference/images/inputvid.mp4: 768x1280 4 persons, Done. (0.028s)\n",
      "video 1/1 (293/414) /notebooks/inference/images/inputvid.mp4: 768x1280 2 persons, Done. (0.021s)\n",
      "video 1/1 (294/414) /notebooks/inference/images/inputvid.mp4: 768x1280 3 persons, Done. (0.021s)\n",
      "video 1/1 (295/414) /notebooks/inference/images/inputvid.mp4: 768x1280 3 persons, Done. (0.021s)\n",
      "video 1/1 (296/414) /notebooks/inference/images/inputvid.mp4: 768x1280 3 persons, Done. (0.021s)\n",
      "video 1/1 (297/414) /notebooks/inference/images/inputvid.mp4: 768x1280 3 persons, Done. (0.021s)\n",
      "video 1/1 (298/414) /notebooks/inference/images/inputvid.mp4: 768x1280 4 persons, 1 traffic lights, Done. (0.023s)\n",
      "video 1/1 (299/414) /notebooks/inference/images/inputvid.mp4: 768x1280 5 persons, Done. (0.025s)\n",
      "video 1/1 (300/414) /notebooks/inference/images/inputvid.mp4: 768x1280 4 persons, Done. (0.022s)\n",
      "video 1/1 (301/414) /notebooks/inference/images/inputvid.mp4: 768x1280 3 persons, Done. (0.023s)\n",
      "video 1/1 (302/414) /notebooks/inference/images/inputvid.mp4: 768x1280 3 persons, Done. (0.021s)\n",
      "video 1/1 (303/414) /notebooks/inference/images/inputvid.mp4: 768x1280 4 persons, Done. (0.022s)\n",
      "video 1/1 (304/414) /notebooks/inference/images/inputvid.mp4: 768x1280 4 persons, Done. (0.021s)\n",
      "video 1/1 (305/414) /notebooks/inference/images/inputvid.mp4: 768x1280 6 persons, Done. (0.022s)\n",
      "video 1/1 (306/414) /notebooks/inference/images/inputvid.mp4: 768x1280 5 persons, Done. (0.021s)\n",
      "video 1/1 (307/414) /notebooks/inference/images/inputvid.mp4: 768x1280 6 persons, Done. (0.023s)\n",
      "video 1/1 (308/414) /notebooks/inference/images/inputvid.mp4: 768x1280 7 persons, Done. (0.022s)\n",
      "video 1/1 (309/414) /notebooks/inference/images/inputvid.mp4: 768x1280 6 persons, Done. (0.023s)\n",
      "video 1/1 (310/414) /notebooks/inference/images/inputvid.mp4: 768x1280 7 persons, Done. (0.023s)\n",
      "video 1/1 (311/414) /notebooks/inference/images/inputvid.mp4: 768x1280 8 persons, Done. (0.021s)\n",
      "video 1/1 (312/414) /notebooks/inference/images/inputvid.mp4: 768x1280 8 persons, Done. (0.021s)\n",
      "video 1/1 (313/414) /notebooks/inference/images/inputvid.mp4: 768x1280 4 persons, Done. (0.021s)\n",
      "video 1/1 (314/414) /notebooks/inference/images/inputvid.mp4: 768x1280 9 persons, Done. (0.021s)\n",
      "video 1/1 (315/414) /notebooks/inference/images/inputvid.mp4: 768x1280 7 persons, Done. (0.021s)\n",
      "video 1/1 (316/414) /notebooks/inference/images/inputvid.mp4: 768x1280 6 persons, Done. (0.021s)\n",
      "video 1/1 (317/414) /notebooks/inference/images/inputvid.mp4: 768x1280 6 persons, Done. (0.022s)\n",
      "video 1/1 (318/414) /notebooks/inference/images/inputvid.mp4: 768x1280 5 persons, Done. (0.022s)\n",
      "video 1/1 (319/414) /notebooks/inference/images/inputvid.mp4: 768x1280 8 persons, Done. (0.021s)\n",
      "video 1/1 (320/414) /notebooks/inference/images/inputvid.mp4: 768x1280 3 persons, Done. (0.021s)\n",
      "video 1/1 (321/414) /notebooks/inference/images/inputvid.mp4: 768x1280 11 persons, 1 handbags, Done. (0.023s)\n",
      "video 1/1 (322/414) /notebooks/inference/images/inputvid.mp4: 768x1280 13 persons, Done. (0.021s)\n",
      "video 1/1 (323/414) /notebooks/inference/images/inputvid.mp4: 768x1280 14 persons, 1 suitcases, Done. (0.021s)\n",
      "video 1/1 (324/414) /notebooks/inference/images/inputvid.mp4: 768x1280 13 persons, Done. (0.021s)\n",
      "video 1/1 (325/414) /notebooks/inference/images/inputvid.mp4: 768x1280 14 persons, 1 suitcases, Done. (0.021s)\n",
      "video 1/1 (326/414) /notebooks/inference/images/inputvid.mp4: 768x1280 13 persons, Done. (0.021s)\n",
      "video 1/1 (327/414) /notebooks/inference/images/inputvid.mp4: 768x1280 12 persons, 1 traffic lights, Done. (0.021s)\n",
      "video 1/1 (328/414) /notebooks/inference/images/inputvid.mp4: 768x1280 6 persons, 1 cars, 1 traffic lights, Done. (0.021s)\n",
      "video 1/1 (329/414) /notebooks/inference/images/inputvid.mp4: 768x1280 8 persons, Done. (0.021s)\n",
      "video 1/1 (330/414) /notebooks/inference/images/inputvid.mp4: 768x1280 9 persons, Done. (0.021s)\n",
      "video 1/1 (331/414) /notebooks/inference/images/inputvid.mp4: 768x1280 7 persons, 1 traffic lights, Done. (0.021s)\n",
      "video 1/1 (332/414) /notebooks/inference/images/inputvid.mp4: 768x1280 6 persons, Done. (0.021s)\n",
      "video 1/1 (333/414) /notebooks/inference/images/inputvid.mp4: 768x1280 5 persons, Done. (0.021s)\n",
      "video 1/1 (334/414) /notebooks/inference/images/inputvid.mp4: 768x1280 7 persons, Done. (0.021s)\n",
      "video 1/1 (335/414) /notebooks/inference/images/inputvid.mp4: 768x1280 4 persons, Done. (0.021s)\n",
      "video 1/1 (336/414) /notebooks/inference/images/inputvid.mp4: 768x1280 5 persons, 1 umbrellas, Done. (0.021s)\n",
      "video 1/1 (337/414) /notebooks/inference/images/inputvid.mp4: 768x1280 6 persons, Done. (0.021s)\n",
      "video 1/1 (338/414) /notebooks/inference/images/inputvid.mp4: 768x1280 8 persons, Done. (0.023s)\n",
      "video 1/1 (339/414) /notebooks/inference/images/inputvid.mp4: 768x1280 9 persons, Done. (0.021s)\n",
      "video 1/1 (340/414) /notebooks/inference/images/inputvid.mp4: 768x1280 7 persons, Done. (0.033s)\n",
      "video 1/1 (341/414) /notebooks/inference/images/inputvid.mp4: 768x1280 7 persons, Done. (0.021s)\n",
      "video 1/1 (342/414) /notebooks/inference/images/inputvid.mp4: 768x1280 6 persons, Done. (0.021s)\n",
      "video 1/1 (343/414) /notebooks/inference/images/inputvid.mp4: 768x1280 6 persons, 1 traffic lights, Done. (0.021s)\n",
      "video 1/1 (344/414) /notebooks/inference/images/inputvid.mp4: 768x1280 6 persons, 1 traffic lights, Done. (0.021s)\n",
      "video 1/1 (345/414) /notebooks/inference/images/inputvid.mp4: 768x1280 6 persons, 1 handbags, Done. (0.021s)\n",
      "video 1/1 (346/414) /notebooks/inference/images/inputvid.mp4: 768x1280 5 persons, Done. (0.021s)\n",
      "video 1/1 (347/414) /notebooks/inference/images/inputvid.mp4: 768x1280 5 persons, 1 traffic lights, 1 handbags, Done. (0.021s)\n",
      "video 1/1 (348/414) /notebooks/inference/images/inputvid.mp4: 768x1280 5 persons, 1 handbags, Done. (0.021s)\n",
      "video 1/1 (349/414) /notebooks/inference/images/inputvid.mp4: 768x1280 4 persons, 1 traffic lights, Done. (0.021s)\n",
      "video 1/1 (350/414) /notebooks/inference/images/inputvid.mp4: 768x1280 5 persons, 1 traffic lights, 1 handbags, Done. (0.021s)\n",
      "video 1/1 (351/414) /notebooks/inference/images/inputvid.mp4: 768x1280 6 persons, 1 traffic lights, 1 handbags, Done. (0.021s)\n",
      "video 1/1 (352/414) /notebooks/inference/images/inputvid.mp4: 768x1280 6 persons, 1 traffic lights, 1 handbags, Done. (0.021s)\n",
      "video 1/1 (353/414) /notebooks/inference/images/inputvid.mp4: 768x1280 6 persons, 1 traffic lights, 1 handbags, Done. (0.021s)\n",
      "video 1/1 (354/414) /notebooks/inference/images/inputvid.mp4: 768x1280 6 persons, 1 handbags, Done. (0.021s)\n",
      "video 1/1 (355/414) /notebooks/inference/images/inputvid.mp4: 768x1280 6 persons, 1 handbags, Done. (0.021s)\n",
      "video 1/1 (356/414) /notebooks/inference/images/inputvid.mp4: 768x1280 7 persons, 1 handbags, Done. (0.021s)\n",
      "video 1/1 (357/414) /notebooks/inference/images/inputvid.mp4: 768x1280 6 persons, 1 handbags, Done. (0.021s)\n",
      "video 1/1 (358/414) /notebooks/inference/images/inputvid.mp4: 768x1280 6 persons, 2 traffic lights, 1 handbags, Done. (0.021s)\n",
      "video 1/1 (359/414) /notebooks/inference/images/inputvid.mp4: 768x1280 5 persons, 1 traffic lights, 1 handbags, Done. (0.021s)\n",
      "video 1/1 (360/414) /notebooks/inference/images/inputvid.mp4: 768x1280 6 persons, 1 handbags, Done. (0.021s)\n",
      "video 1/1 (361/414) /notebooks/inference/images/inputvid.mp4: 768x1280 6 persons, 1 handbags, Done. (0.021s)\n",
      "video 1/1 (362/414) /notebooks/inference/images/inputvid.mp4: 768x1280 5 persons, 1 handbags, Done. (0.021s)\n",
      "video 1/1 (363/414) /notebooks/inference/images/inputvid.mp4: 768x1280 3 persons, 1 handbags, 1 cell phones, Done. (0.021s)\n",
      "video 1/1 (364/414) /notebooks/inference/images/inputvid.mp4: 768x1280 5 persons, 1 handbags, 1 cell phones, Done. (0.021s)\n",
      "video 1/1 (365/414) /notebooks/inference/images/inputvid.mp4: 768x1280 5 persons, Done. (0.021s)\n",
      "video 1/1 (366/414) /notebooks/inference/images/inputvid.mp4: 768x1280 5 persons, 1 handbags, Done. (0.024s)\n",
      "video 1/1 (367/414) /notebooks/inference/images/inputvid.mp4: 768x1280 4 persons, 1 handbags, Done. (0.021s)\n",
      "video 1/1 (368/414) /notebooks/inference/images/inputvid.mp4: 768x1280 5 persons, 1 handbags, Done. (0.021s)\n",
      "video 1/1 (369/414) /notebooks/inference/images/inputvid.mp4: 768x1280 3 persons, 1 handbags, Done. (0.021s)\n",
      "video 1/1 (370/414) /notebooks/inference/images/inputvid.mp4: 768x1280 3 persons, 1 handbags, Done. (0.021s)\n",
      "video 1/1 (371/414) /notebooks/inference/images/inputvid.mp4: 768x1280 3 persons, 1 handbags, Done. (0.021s)\n",
      "video 1/1 (372/414) /notebooks/inference/images/inputvid.mp4: 768x1280 5 persons, 1 handbags, Done. (0.021s)\n",
      "video 1/1 (373/414) /notebooks/inference/images/inputvid.mp4: 768x1280 3 persons, 1 handbags, Done. (0.021s)\n",
      "video 1/1 (374/414) /notebooks/inference/images/inputvid.mp4: 768x1280 5 persons, 1 handbags, Done. (0.021s)\n",
      "video 1/1 (375/414) /notebooks/inference/images/inputvid.mp4: 768x1280 5 persons, Done. (0.021s)\n",
      "video 1/1 (376/414) /notebooks/inference/images/inputvid.mp4: 768x1280 4 persons, Done. (0.021s)\n",
      "video 1/1 (377/414) /notebooks/inference/images/inputvid.mp4: 768x1280 5 persons, Done. (0.021s)\n",
      "video 1/1 (378/414) /notebooks/inference/images/inputvid.mp4: 768x1280 7 persons, 1 handbags, Done. (0.021s)\n",
      "video 1/1 (379/414) /notebooks/inference/images/inputvid.mp4: 768x1280 8 persons, 1 handbags, 1 suitcases, Done. (0.022s)\n",
      "video 1/1 (380/414) /notebooks/inference/images/inputvid.mp4: 768x1280 4 persons, 1 handbags, 2 suitcases, Done. (0.021s)\n",
      "video 1/1 (381/414) /notebooks/inference/images/inputvid.mp4: 768x1280 7 persons, 1 handbags, 1 suitcases, Done. (0.021s)\n",
      "video 1/1 (382/414) /notebooks/inference/images/inputvid.mp4: 768x1280 6 persons, Done. (0.021s)\n",
      "video 1/1 (383/414) /notebooks/inference/images/inputvid.mp4: 768x1280 5 persons, 1 handbags, Done. (0.021s)\n",
      "video 1/1 (384/414) /notebooks/inference/images/inputvid.mp4: 768x1280 5 persons, 1 suitcases, Done. (0.021s)\n",
      "video 1/1 (385/414) /notebooks/inference/images/inputvid.mp4: 768x1280 5 persons, 1 cars, 1 suitcases, Done. (0.021s)\n",
      "video 1/1 (386/414) /notebooks/inference/images/inputvid.mp4: 768x1280 6 persons, 1 suitcases, Done. (0.021s)\n",
      "video 1/1 (387/414) /notebooks/inference/images/inputvid.mp4: 768x1280 4 persons, 1 suitcases, Done. (0.021s)\n",
      "video 1/1 (388/414) /notebooks/inference/images/inputvid.mp4: 768x1280 5 persons, Done. (0.021s)\n",
      "video 1/1 (389/414) /notebooks/inference/images/inputvid.mp4: 768x1280 6 persons, Done. (0.021s)\n",
      "video 1/1 (390/414) /notebooks/inference/images/inputvid.mp4: 768x1280 6 persons, Done. (0.022s)\n",
      "video 1/1 (391/414) /notebooks/inference/images/inputvid.mp4: 768x1280 6 persons, 1 cars, Done. (0.021s)\n",
      "video 1/1 (392/414) /notebooks/inference/images/inputvid.mp4: 768x1280 7 persons, Done. (0.021s)\n",
      "video 1/1 (393/414) /notebooks/inference/images/inputvid.mp4: 768x1280 6 persons, 1 cars, Done. (0.021s)\n",
      "video 1/1 (394/414) /notebooks/inference/images/inputvid.mp4: 768x1280 4 persons, 1 ties, Done. (0.021s)\n",
      "video 1/1 (395/414) /notebooks/inference/images/inputvid.mp4: 768x1280 5 persons, 1 ties, Done. (0.021s)\n",
      "video 1/1 (396/414) /notebooks/inference/images/inputvid.mp4: 768x1280 4 persons, 1 ties, Done. (0.021s)\n",
      "video 1/1 (397/414) /notebooks/inference/images/inputvid.mp4: 768x1280 7 persons, Done. (0.021s)\n",
      "video 1/1 (398/414) /notebooks/inference/images/inputvid.mp4: 768x1280 7 persons, Done. (0.021s)\n",
      "video 1/1 (399/414) /notebooks/inference/images/inputvid.mp4: 768x1280 6 persons, Done. (0.021s)\n",
      "video 1/1 (400/414) /notebooks/inference/images/inputvid.mp4: 768x1280 8 persons, Done. (0.021s)\n",
      "video 1/1 (401/414) /notebooks/inference/images/inputvid.mp4: 768x1280 7 persons, Done. (0.022s)\n",
      "video 1/1 (402/414) /notebooks/inference/images/inputvid.mp4: 768x1280 7 persons, Done. (0.021s)\n",
      "video 1/1 (403/414) /notebooks/inference/images/inputvid.mp4: 768x1280 6 persons, 1 cars, Done. (0.022s)\n",
      "video 1/1 (404/414) /notebooks/inference/images/inputvid.mp4: 768x1280 5 persons, Done. (0.021s)\n",
      "video 1/1 (405/414) /notebooks/inference/images/inputvid.mp4: 768x1280 3 persons, 1 backpacks, Done. (0.021s)\n",
      "video 1/1 (406/414) /notebooks/inference/images/inputvid.mp4: 768x1280 4 persons, 1 cell phones, Done. (0.021s)\n",
      "video 1/1 (407/414) /notebooks/inference/images/inputvid.mp4: 768x1280 4 persons, Done. (0.021s)\n",
      "video 1/1 (408/414) /notebooks/inference/images/inputvid.mp4: 768x1280 8 persons, Done. (0.021s)\n",
      "video 1/1 (409/414) /notebooks/inference/images/inputvid.mp4: 768x1280 6 persons, 1 suitcases, Done. (0.021s)\n",
      "video 1/1 (410/414) /notebooks/inference/images/inputvid.mp4: 768x1280 5 persons, 1 cars, Done. (0.021s)\n",
      "video 1/1 (411/414) /notebooks/inference/images/inputvid.mp4: 768x1280 6 persons, Done. (0.021s)\n",
      "video 1/1 (412/414) /notebooks/inference/images/inputvid.mp4: 768x1280 6 persons, Done. (0.021s)\n",
      "video 1/1 (413/414) /notebooks/inference/images/inputvid.mp4: 768x1280 7 persons, Done. (0.021s)\n",
      "video 1/1 (414/414) /notebooks/inference/images/inputvid.mp4: 768x1280 6 persons, 1 buss, 1 suitcases, Done. (0.021s)\n",
      "Results saved to inference/output\n",
      "Done. (13.823s)\n",
      "CPU times: user 226 ms, sys: 86.2 ms, total: 312 ms\n",
      "Wall time: 21.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!python detect.py --source inference/images/inputvid.mp4 --cfg cfg/yolor_csp_x.cfg --weights yolor_csp_x.pt --conf 0.25 --img-size 1280 --device 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a358e81-ad9a-471a-a9d8-74ae65084533",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-26T00:47:04.725479Z",
     "iopub.status.busy": "2022-04-26T00:47:04.724805Z",
     "iopub.status.idle": "2022-04-26T00:47:36.564977Z",
     "shell.execute_reply": "2022-04-26T00:47:36.564444Z",
     "shell.execute_reply.started": "2022-04-26T00:47:04.725417Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using torch 1.11.0a0+17540c5 CUDA:0 (RTX A4000, 16117MB)\n",
      "\n",
      "Namespace(adam=False, batch_size=2, bucket='', cache_images=True, cfg='cfg/yolor_csp_x.cfg', data='data/coco.yaml', device='0', epochs=300, evolve=False, exist_ok=False, global_rank=-1, hyp='data/hyp.scratch.1280.yaml', image_weights=False, img_size=[1280, 1280], local_rank=-1, log_imgs=16, multi_scale=False, name='yolor_csp_x_run1', noautoanchor=False, nosave=False, notest=False, project='runs/train', rect=False, resume=False, save_dir='runs/train/yolor_csp_x_run16', single_cls=False, sync_bn=False, total_batch_size=2, weights='', workers=8, world_size=1)\n",
      "Start Tensorboard with \"tensorboard --logdir runs/train\", view at http://localhost:6006/\n",
      "Hyperparameters {'lr0': 0.01, 'lrf': 0.2, 'momentum': 0.937, 'weight_decay': 0.0005, 'warmup_epochs': 3.0, 'warmup_momentum': 0.8, 'warmup_bias_lr': 0.1, 'box': 0.05, 'cls': 0.5, 'cls_pw': 1.0, 'obj': 1.0, 'obj_pw': 1.0, 'iou_t': 0.2, 'anchor_t': 4.0, 'fl_gamma': 0.0, 'hsv_h': 0.015, 'hsv_s': 0.7, 'hsv_v': 0.4, 'degrees': 0.0, 'translate': 0.5, 'scale': 0.5, 'shear': 0.0, 'perspective': 0.0, 'flipud': 0.0, 'fliplr': 0.5, 'mosaic': 1.0, 'mixup': 0.0}\n",
      "Model Summary: 623 layers, 99754058 parameters, 99754058 gradients\n",
      "Optimizer groups: 137 .bias, 137 conv.weight, 140 other\n",
      "Scanning labels ../coco/labels/train2017.cache3 (117266 found, 0 missing, 1021 empty, 0 duplicate, for 118287 images): 118287it [00:05, 19880.07it/s]\n",
      "Caching images (40.8GB):  10%|         | 11544/118287 [00:15<02:49, 631.28it/s]"
     ]
    }
   ],
   "source": [
    "!python train.py --batch-size 2 --img 1280 1280 --data data/coco.yaml --cfg cfg/yolor_csp_x.cfg --weights '' --device 0 --name yolor_csp_x_run1 --hyp data/hyp.scratch.1280.yaml --epochs 300\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6de12c3-2300-4691-9781-7055a1f9821a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-27T00:42:54.460636Z",
     "iopub.status.busy": "2022-04-27T00:42:54.460406Z",
     "iopub.status.idle": "2022-04-27T00:44:44.255501Z",
     "shell.execute_reply": "2022-04-27T00:44:44.254935Z",
     "shell.execute_reply.started": "2022-04-27T00:42:54.460610Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using torch 1.11.0a0+17540c5 CUDA:0 (RTX A6000, 48685MB)\n",
      "\n",
      "Namespace(adam=False, batch_size=2, bucket='', cache_images=True, cfg='cfg/yolor_csp_x.cfg', data='data/coco.yaml', device='0', epochs=300, evolve=False, exist_ok=False, global_rank=-1, hyp='data/hyp.scratch.1280.yaml', image_weights=False, img_size=[1280, 1280], local_rank=-1, log_imgs=16, multi_scale=False, name='yolor_csp_x_', noautoanchor=False, nosave=False, notest=False, project='runs/train', rect=False, resume=False, save_dir='runs/train/yolor_csp_x_', single_cls=False, sync_bn=False, total_batch_size=2, weights='', workers=8, world_size=1)\n",
      "Start Tensorboard with \"tensorboard --logdir runs/train\", view at http://localhost:6006/\n",
      "Hyperparameters {'lr0': 0.01, 'lrf': 0.2, 'momentum': 0.937, 'weight_decay': 0.0005, 'warmup_epochs': 3.0, 'warmup_momentum': 0.8, 'warmup_bias_lr': 0.1, 'box': 0.05, 'cls': 0.5, 'cls_pw': 1.0, 'obj': 1.0, 'obj_pw': 1.0, 'iou_t': 0.2, 'anchor_t': 4.0, 'fl_gamma': 0.0, 'hsv_h': 0.015, 'hsv_s': 0.7, 'hsv_v': 0.4, 'degrees': 0.0, 'translate': 0.5, 'scale': 0.5, 'shear': 0.0, 'perspective': 0.0, 'flipud': 0.0, 'fliplr': 0.5, 'mosaic': 1.0, 'mixup': 0.0}\n",
      "Model Summary: 623 layers, 99754058 parameters, 99754058 gradients\n",
      "Optimizer groups: 137 .bias, 137 conv.weight, 140 other\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"train.py\", line 537, in <module>\n",
      "    train(hyp, opt, device, tb_writer, wandb)\n",
      "  File \"train.py\", line 181, in train\n",
      "    dataloader, dataset = create_dataloader(train_path, imgsz, batch_size, gs, opt,\n",
      "  File \"/notebooks/utils/datasets.py\", line 64, in create_dataloader\n",
      "    dataset = LoadImagesAndLabels(path, imgsz, batch_size,\n",
      "  File \"/notebooks/utils/datasets.py\", line 397, in __init__\n",
      "    if cache['hash'] != get_hash(self.label_files + self.img_files):  # dataset changed\n",
      "  File \"/notebooks/utils/datasets.py\", line 42, in get_hash\n",
      "    return sum(os.path.getsize(f) for f in files if os.path.isfile(f))\n",
      "  File \"/notebooks/utils/datasets.py\", line 42, in <genexpr>\n",
      "    return sum(os.path.getsize(f) for f in files if os.path.isfile(f))\n",
      "  File \"/opt/conda/lib/python3.8/genericpath.py\", line 30, in isfile\n",
      "    st = os.stat(path)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python train.py --batch-size 2 --img 1280 1280 --data data/coco.yaml --cfg cfg/yolor_csp_x.cfg --weights '' --device 0 --name yolor_csp_x_ --hyp data/hyp.scratch.1280.yaml --epochs 300\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d9cfe71-2a1c-4edb-b9d2-b1142d5e8838",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-27T00:45:30.594679Z",
     "iopub.status.busy": "2022-04-27T00:45:30.594100Z",
     "iopub.status.idle": "2022-04-27T00:45:42.434586Z",
     "shell.execute_reply": "2022-04-27T00:45:42.433846Z",
     "shell.execute_reply.started": "2022-04-27T00:45:30.594648Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(agnostic_nms=False, augment=False, cfg='cfg/yolor_csp_x.cfg', classes=None, conf_thres=0.25, device='0', img_size=1280, iou_thres=0.5, names='data/coco.names', output='inference/output', save_txt=False, source='inference/images/horses.jpg', update=False, view_img=False, weights=['runs/train/yolor_csp_x_run110/weights/best_ap50.pt'])\n",
      "/opt/conda/lib/python3.8/site-packages/torch/functional.py:568: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  /opt/pytorch/pytorch/aten/src/ATen/native/TensorShape.cpp:2166.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "image 1/1 /notebooks/inference/images/horses.jpg: 896x1280 Done. (0.102s)\n",
      "Results saved to inference/output\n",
      "Done. (3.752s)\n"
     ]
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d09487-2f5f-45ed-952d-a4a6cbb58ec2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
